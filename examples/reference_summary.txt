**ABSTRACT**
This paper introduces a two-phase hierarchical framework for automatic summarization of long scientific documents using pre-trained models without fine-tuning. The system combines keyphrase extraction, contrastive learning, and large language models to generate summaries with 75-80% compression while preserving scientific terminology and numerical results. Evaluation on 100+ PubMed articles shows ROUGE-L F1 scores of 0.40-0.45 and BERTScore F1 of 0.84-0.88.

**INTRODUCTION**
Scientific literature growth creates challenges for researchers to stay current with relevant findings. Existing summarization approaches either produce disjointed extractive summaries or require extensive fine-tuning for abstractive generation. This framework addresses these limitations through hierarchical processing that respects document structure, handles length constraints, and maintains scientific accuracy.

**METHODS**
The framework processes documents through five components: preprocessing with section detection and scientific text cleaning, KeyBERT-based keyphrase extraction with dynamic calculation, contrastive learning using InfoNCE loss to enhance embeddings, section importance scoring via semantic similarity, and BART-based abstractive summarization. The system operates in zero-shot mode without domain-specific training.

**RESULTS**
Evaluation on 100 PubMed articles achieved section-level ROUGE-L F1 of 0.45 and document-level ROUGE-L F1 of 0.40, with BERTScore F1 of 0.88 and 0.84 respectively. Compression ratios reached 75-80%. Ablation studies confirmed contributions of contrastive learning and keyphrase extraction. Processing time averaged 45 seconds per document on CPU.

**DISCUSSION**
Results demonstrate effective self-reliant summarization without fine-tuning. Strong BERTScore indicates good semantic content capture despite surface-level differences. Contrastive learning enhances representations, while exponential allocation balances summary space across sections. Limitations include reliance on section structure and English-only support.

**CONCLUSION**
The framework successfully summarizes long scientific documents through hierarchical processing combining multiple pre-trained components. Strong performance metrics and efficient operation make it practical for processing large literature volumes. The approach preserves scientific rigor while achieving substantial compression.
